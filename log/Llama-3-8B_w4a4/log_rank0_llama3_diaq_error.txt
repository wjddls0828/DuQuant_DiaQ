[2025-05-13 22:59:04 root] (main.py 345): INFO Namespace(rts=0.5, my_file_name='llama3_diaq_error', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 22:59:07 root] (main.py 429): INFO === start quantization ===
[2025-05-13 22:59:07 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 22:59:07 root] (duquant.py 45): INFO Starting ...
[2025-05-13 22:59:09 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 22:59:12 root] (duquant.py 171): INFO === Start quantize layer 1 ===
[2025-05-13 22:59:14 root] (duquant.py 171): INFO === Start quantize layer 2 ===
[2025-05-13 22:59:16 root] (duquant.py 171): INFO === Start quantize layer 3 ===
[2025-05-13 22:59:19 root] (duquant.py 171): INFO === Start quantize layer 4 ===
[2025-05-13 22:59:21 root] (duquant.py 171): INFO === Start quantize layer 5 ===
[2025-05-13 22:59:23 root] (duquant.py 171): INFO === Start quantize layer 6 ===
[2025-05-13 22:59:26 root] (duquant.py 171): INFO === Start quantize layer 7 ===
[2025-05-13 22:59:29 root] (duquant.py 171): INFO === Start quantize layer 8 ===
[2025-05-13 22:59:31 root] (duquant.py 171): INFO === Start quantize layer 9 ===
[2025-05-13 22:59:34 root] (duquant.py 171): INFO === Start quantize layer 10 ===
[2025-05-13 22:59:36 root] (duquant.py 171): INFO === Start quantize layer 11 ===
[2025-05-13 22:59:39 root] (duquant.py 171): INFO === Start quantize layer 12 ===
[2025-05-13 22:59:41 root] (duquant.py 171): INFO === Start quantize layer 13 ===
[2025-05-13 22:59:44 root] (duquant.py 171): INFO === Start quantize layer 14 ===
[2025-05-13 22:59:46 root] (duquant.py 171): INFO === Start quantize layer 15 ===
[2025-05-13 22:59:49 root] (duquant.py 171): INFO === Start quantize layer 16 ===
[2025-05-13 22:59:51 root] (duquant.py 171): INFO === Start quantize layer 17 ===
[2025-05-13 22:59:54 root] (duquant.py 171): INFO === Start quantize layer 18 ===
[2025-05-13 22:59:57 root] (duquant.py 171): INFO === Start quantize layer 19 ===
[2025-05-13 22:59:59 root] (duquant.py 171): INFO === Start quantize layer 20 ===
[2025-05-13 23:00:02 root] (duquant.py 171): INFO === Start quantize layer 21 ===
[2025-05-13 23:00:04 root] (duquant.py 171): INFO === Start quantize layer 22 ===
[2025-05-13 23:00:07 root] (duquant.py 171): INFO === Start quantize layer 23 ===
[2025-05-13 23:00:09 root] (duquant.py 171): INFO === Start quantize layer 24 ===
[2025-05-13 23:00:12 root] (duquant.py 171): INFO === Start quantize layer 25 ===
[2025-05-13 23:00:14 root] (duquant.py 171): INFO === Start quantize layer 26 ===
[2025-05-13 23:00:17 root] (duquant.py 171): INFO === Start quantize layer 27 ===
[2025-05-13 23:00:20 root] (duquant.py 171): INFO === Start quantize layer 28 ===
[2025-05-13 23:00:22 root] (duquant.py 171): INFO === Start quantize layer 29 ===
[2025-05-13 23:00:25 root] (duquant.py 171): INFO === Start quantize layer 30 ===
[2025-05-13 23:00:27 root] (duquant.py 171): INFO === Start quantize layer 31 ===
[2025-05-13 23:00:30 root] (main.py 458): INFO 82.70453310012817
[2025-05-13 23:00:30 root] (main.py 481): INFO {'self_attn.k_proj': {'x_l2_diaq': 0.7205058271065354, 'x_relnorm_diaq': 0.12142264051362872, 'x_cos_diaq': 0.9926720336079597, 'x_l2_rtn': 0.7205058271065354, 'x_relnorm_rtn': 0.12142264051362872, 'x_cos_rtn': 0.9926713891327381, 'wx_l2_diaq': 2.5298081561923027, 'wx_relnorm_diaq': 0.0348421297094319, 'wx_cos_diaq': 0.9993957467377186, 'wx_l2_rtn': 2.4902523681521416, 'wx_relnorm_rtn': 0.034297054866328835, 'wx_cos_rtn': 0.9993957262486219}, 'self_attn.v_proj': {'x_l2_diaq': 0.7205058271065354, 'x_relnorm_diaq': 0.12142264051362872, 'x_cos_diaq': 0.9926720336079597, 'x_l2_rtn': 0.7205058271065354, 'x_relnorm_rtn': 0.12142264051362872, 'x_cos_rtn': 0.9926713891327381, 'wx_l2_diaq': 0.9023397462442517, 'wx_relnorm_diaq': 0.11035050777718425, 'wx_cos_diaq': 0.9937776606529951, 'wx_l2_rtn': 0.9060475411824882, 'wx_relnorm_rtn': 0.11080941488035023, 'wx_cos_rtn': 0.9937771055847406}, 'self_attn.q_proj': {'x_l2_diaq': 0.7205058271065354, 'x_relnorm_diaq': 0.12142264051362872, 'x_cos_diaq': 0.9926720336079597, 'x_l2_rtn': 0.7205058271065354, 'x_relnorm_rtn': 0.12142264051362872, 'x_cos_rtn': 0.9926713891327381, 'wx_l2_diaq': 3.176753059029579, 'wx_relnorm_diaq': 0.04341951388050802, 'wx_cos_diaq': 0.9990323055535555, 'wx_l2_rtn': 3.142503285780549, 'wx_relnorm_rtn': 0.04303980965050869, 'wx_cos_rtn': 0.9990322533994913}, 'self_attn.o_proj': {'x_l2_diaq': 0.23889359715394676, 'x_relnorm_diaq': 0.12100234813988209, 'x_cos_diaq': 0.9927173275500536, 'x_l2_rtn': 0.23889359715394676, 'x_relnorm_rtn': 0.12100234813988209, 'x_cos_rtn': 0.9927167389541864, 'wx_l2_diaq': 0.4765541763044894, 'wx_relnorm_diaq': 0.15402692998759449, 'wx_cos_diaq': 0.9877145495265722, 'wx_l2_rtn': 0.47655322798527777, 'wx_relnorm_rtn': 0.15402785735204816, 'wx_cos_rtn': 0.9877134226262569}, 'mlp.gate_proj': {'x_l2_diaq': 0.5206929938867688, 'x_relnorm_diaq': 0.12167306221090257, 'x_cos_diaq': 0.9926417954266071, 'x_l2_rtn': 0.5206929938867688, 'x_relnorm_rtn': 0.12167306221090257, 'x_cos_rtn': 0.9926412478089333, 'wx_l2_diaq': 3.9371017906814814, 'wx_relnorm_diaq': 0.08047183533199131, 'wx_cos_diaq': 0.9966754149645567, 'wx_l2_rtn': 3.9443894792348146, 'wx_relnorm_rtn': 0.08065960020758212, 'wx_cos_rtn': 0.9966751877218485}, 'mlp.down_proj': {'x_l2_diaq': 0.20093120681121945, 'x_relnorm_diaq': 0.13779088854789734, 'x_cos_diaq': 0.9905861634761095, 'x_l2_rtn': 0.20093120681121945, 'x_relnorm_rtn': 0.13779088854789734, 'x_cos_rtn': 0.9905851557850838, 'wx_l2_diaq': 0.7211389308795333, 'wx_relnorm_diaq': 0.18153286771848798, 'wx_cos_diaq': 0.9830185230821371, 'wx_l2_rtn': 0.7255586814135313, 'wx_relnorm_rtn': 0.1829309337772429, 'wx_cos_rtn': 0.9830168969929218}, 'mlp.up_proj': {'x_l2_diaq': 0.5206929938867688, 'x_relnorm_diaq': 0.12167306221090257, 'x_cos_diaq': 0.9926417954266071, 'x_l2_rtn': 0.5206929938867688, 'x_relnorm_rtn': 0.12167306221090257, 'x_cos_rtn': 0.9926412478089333, 'wx_l2_diaq': 0.8688440937548876, 'wx_relnorm_diaq': 0.11115677375346422, 'wx_cos_diaq': 0.9935993179678917, 'wx_l2_rtn': 0.8722849637269974, 'wx_relnorm_rtn': 0.11162142548710108, 'wx_cos_rtn': 0.9935988765209913}}
[2025-05-13 23:00:30 root] (main.py 496): INFO {'x_l2_diaq': 0.5203897532940444, 'x_relnorm_diaq': 0.12377246895006724, 'x_cos_diaq': 0.9923718832433224, 'x_l2_rtn': 0.5203897532940444, 'x_relnorm_rtn': 0.12377246895006724, 'x_cos_rtn': 0.9923712225364787, 'wx_l2_diaq': 1.8017914218695037, 'wx_relnorm_diaq': 0.1022572225940946, 'wx_cos_diaq': 0.9933162169264895, 'wx_l2_rtn': 1.7939413639251143, 'wx_relnorm_rtn': 0.10248372803159457, 'wx_cos_rtn': 0.9933156384421247}
[2025-05-13 23:00:39 root] (main.py 105): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-05-13 23:07:41 root] (main.py 143): INFO wikitext2 : 8.12136173248291
[2025-05-13 23:18:22 root] (main.py 345): INFO Namespace(rts=0.5, my_file_name='llama3_diaq_error', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 23:18:26 root] (main.py 429): INFO === start quantization ===
[2025-05-13 23:18:26 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 23:18:26 root] (duquant.py 45): INFO Starting ...
[2025-05-13 23:18:28 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 23:18:31 root] (duquant.py 171): INFO === Start quantize layer 1 ===
[2025-05-13 23:18:33 root] (duquant.py 171): INFO === Start quantize layer 2 ===
[2025-05-13 23:18:35 root] (duquant.py 171): INFO === Start quantize layer 3 ===
[2025-05-13 23:18:37 root] (duquant.py 171): INFO === Start quantize layer 4 ===
[2025-05-13 23:18:39 root] (duquant.py 171): INFO === Start quantize layer 5 ===
[2025-05-13 23:18:42 root] (duquant.py 171): INFO === Start quantize layer 6 ===
[2025-05-13 23:18:44 root] (duquant.py 171): INFO === Start quantize layer 7 ===
[2025-05-13 23:18:47 root] (duquant.py 171): INFO === Start quantize layer 8 ===
[2025-05-13 23:18:49 root] (duquant.py 171): INFO === Start quantize layer 9 ===
[2025-05-13 23:18:52 root] (duquant.py 171): INFO === Start quantize layer 10 ===
[2025-05-13 23:18:55 root] (duquant.py 171): INFO === Start quantize layer 11 ===
[2025-05-13 23:18:57 root] (duquant.py 171): INFO === Start quantize layer 12 ===
[2025-05-13 23:19:00 root] (duquant.py 171): INFO === Start quantize layer 13 ===
[2025-05-13 23:19:03 root] (duquant.py 171): INFO === Start quantize layer 14 ===
[2025-05-13 23:19:05 root] (duquant.py 171): INFO === Start quantize layer 15 ===
[2025-05-13 23:19:08 root] (duquant.py 171): INFO === Start quantize layer 16 ===
[2025-05-13 23:19:10 root] (duquant.py 171): INFO === Start quantize layer 17 ===
[2025-05-13 23:19:13 root] (duquant.py 171): INFO === Start quantize layer 18 ===
[2025-05-13 23:19:15 root] (duquant.py 171): INFO === Start quantize layer 19 ===
[2025-05-13 23:19:18 root] (duquant.py 171): INFO === Start quantize layer 20 ===
[2025-05-13 23:19:20 root] (duquant.py 171): INFO === Start quantize layer 21 ===
[2025-05-13 23:19:23 root] (duquant.py 171): INFO === Start quantize layer 22 ===
[2025-05-13 23:19:25 root] (duquant.py 171): INFO === Start quantize layer 23 ===
[2025-05-13 23:19:28 root] (duquant.py 171): INFO === Start quantize layer 24 ===
[2025-05-13 23:19:31 root] (duquant.py 171): INFO === Start quantize layer 25 ===
[2025-05-13 23:19:33 root] (duquant.py 171): INFO === Start quantize layer 26 ===
[2025-05-13 23:19:36 root] (duquant.py 171): INFO === Start quantize layer 27 ===
[2025-05-13 23:19:38 root] (duquant.py 171): INFO === Start quantize layer 28 ===
[2025-05-13 23:19:41 root] (duquant.py 171): INFO === Start quantize layer 29 ===
[2025-05-13 23:19:43 root] (duquant.py 171): INFO === Start quantize layer 30 ===
[2025-05-13 23:19:46 root] (duquant.py 171): INFO === Start quantize layer 31 ===
[2025-05-13 23:19:49 root] (main.py 458): INFO 82.87814998626709
[2025-05-13 23:19:49 root] (main.py 481): INFO {'self_attn.k_proj': {'x_l2_diaq': 0.7205058271065354, 'x_relnorm_diaq': 0.12142264051362872, 'x_cos_diaq': 0.9926720336079597, 'x_l2_rtn': 0.7205058271065354, 'x_relnorm_rtn': 0.12142264051362872, 'x_cos_rtn': 0.9926713891327381, 'wx_l2_diaq': 2.5298081561923027, 'wx_relnorm_diaq': 0.0348421297094319, 'wx_cos_diaq': 0.9993957467377186, 'wx_l2_rtn': 2.4902523681521416, 'wx_relnorm_rtn': 0.034297054866328835, 'wx_cos_rtn': 0.9993957262486219}, 'self_attn.v_proj': {'x_l2_diaq': 0.7205058271065354, 'x_relnorm_diaq': 0.12142264051362872, 'x_cos_diaq': 0.9926720336079597, 'x_l2_rtn': 0.7205058271065354, 'x_relnorm_rtn': 0.12142264051362872, 'x_cos_rtn': 0.9926713891327381, 'wx_l2_diaq': 0.9023397462442517, 'wx_relnorm_diaq': 0.11035050777718425, 'wx_cos_diaq': 0.9937776606529951, 'wx_l2_rtn': 0.9060475411824882, 'wx_relnorm_rtn': 0.11080941488035023, 'wx_cos_rtn': 0.9937771055847406}, 'self_attn.q_proj': {'x_l2_diaq': 0.7205058271065354, 'x_relnorm_diaq': 0.12142264051362872, 'x_cos_diaq': 0.9926720336079597, 'x_l2_rtn': 0.7205058271065354, 'x_relnorm_rtn': 0.12142264051362872, 'x_cos_rtn': 0.9926713891327381, 'wx_l2_diaq': 3.176753059029579, 'wx_relnorm_diaq': 0.04341951388050802, 'wx_cos_diaq': 0.9990323055535555, 'wx_l2_rtn': 3.142503285780549, 'wx_relnorm_rtn': 0.04303980965050869, 'wx_cos_rtn': 0.9990322533994913}, 'self_attn.o_proj': {'x_l2_diaq': 0.23889359715394676, 'x_relnorm_diaq': 0.12100234813988209, 'x_cos_diaq': 0.9927173275500536, 'x_l2_rtn': 0.23889359715394676, 'x_relnorm_rtn': 0.12100234813988209, 'x_cos_rtn': 0.9927167389541864, 'wx_l2_diaq': 0.4765541763044894, 'wx_relnorm_diaq': 0.15402692998759449, 'wx_cos_diaq': 0.9877145495265722, 'wx_l2_rtn': 0.47655322798527777, 'wx_relnorm_rtn': 0.15402785735204816, 'wx_cos_rtn': 0.9877134226262569}, 'mlp.gate_proj': {'x_l2_diaq': 0.5206929938867688, 'x_relnorm_diaq': 0.12167306221090257, 'x_cos_diaq': 0.9926417954266071, 'x_l2_rtn': 0.5206929938867688, 'x_relnorm_rtn': 0.12167306221090257, 'x_cos_rtn': 0.9926412478089333, 'wx_l2_diaq': 3.9371017906814814, 'wx_relnorm_diaq': 0.08047183533199131, 'wx_cos_diaq': 0.9966754149645567, 'wx_l2_rtn': 3.9443894792348146, 'wx_relnorm_rtn': 0.08065960020758212, 'wx_cos_rtn': 0.9966751877218485}, 'mlp.down_proj': {'x_l2_diaq': 0.20093120681121945, 'x_relnorm_diaq': 0.13779088854789734, 'x_cos_diaq': 0.9905861634761095, 'x_l2_rtn': 0.20093120681121945, 'x_relnorm_rtn': 0.13779088854789734, 'x_cos_rtn': 0.9905851557850838, 'wx_l2_diaq': 0.7211389308795333, 'wx_relnorm_diaq': 0.18153286771848798, 'wx_cos_diaq': 0.9830185230821371, 'wx_l2_rtn': 0.7255586814135313, 'wx_relnorm_rtn': 0.1829309337772429, 'wx_cos_rtn': 0.9830168969929218}, 'mlp.up_proj': {'x_l2_diaq': 0.5206929938867688, 'x_relnorm_diaq': 0.12167306221090257, 'x_cos_diaq': 0.9926417954266071, 'x_l2_rtn': 0.5206929938867688, 'x_relnorm_rtn': 0.12167306221090257, 'x_cos_rtn': 0.9926412478089333, 'wx_l2_diaq': 0.8688440937548876, 'wx_relnorm_diaq': 0.11115677375346422, 'wx_cos_diaq': 0.9935993179678917, 'wx_l2_rtn': 0.8722849637269974, 'wx_relnorm_rtn': 0.11162142548710108, 'wx_cos_rtn': 0.9935988765209913}}
[2025-05-13 23:19:49 root] (main.py 496): INFO {'x_l2_diaq': 0.5203897532940444, 'x_relnorm_diaq': 0.12377246895006724, 'x_cos_diaq': 0.9923718832433224, 'x_l2_rtn': 0.5203897532940444, 'x_relnorm_rtn': 0.12377246895006724, 'x_cos_rtn': 0.9923712225364787, 'wx_l2_diaq': 1.8017914218695037, 'wx_relnorm_diaq': 0.1022572225940946, 'wx_cos_diaq': 0.9933162169264895, 'wx_l2_rtn': 1.7939413639251143, 'wx_relnorm_rtn': 0.10248372803159457, 'wx_cos_rtn': 0.9933156384421247}
[2025-05-13 23:19:59 root] (main.py 105): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-05-13 23:28:25 root] (main.py 345): INFO Namespace(rts=0.5, my_file_name='llama3_diaq_error', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 23:28:27 root] (main.py 429): INFO === start quantization ===
[2025-05-13 23:29:59 root] (main.py 345): INFO Namespace(rts=0.5, my_file_name='llama3_diaq_error', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 23:30:01 root] (main.py 429): INFO === start quantization ===
[2025-05-13 23:30:01 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 23:30:01 root] (duquant.py 45): INFO Starting ...
[2025-05-13 23:30:04 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 23:30:29 root] (main.py 345): INFO Namespace(rts=0.5, my_file_name='llama3_diaq_error', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 23:30:32 root] (main.py 429): INFO === start quantization ===
[2025-05-13 23:30:32 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 23:30:32 root] (duquant.py 45): INFO Starting ...
[2025-05-13 23:30:35 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 23:30:39 root] (duquant.py 171): INFO === Start quantize layer 1 ===
[2025-05-13 23:30:42 root] (duquant.py 171): INFO === Start quantize layer 2 ===
[2025-05-13 23:30:45 root] (duquant.py 171): INFO === Start quantize layer 3 ===
[2025-05-13 23:30:48 root] (duquant.py 171): INFO === Start quantize layer 4 ===
[2025-05-13 23:30:50 root] (duquant.py 171): INFO === Start quantize layer 5 ===
[2025-05-13 23:30:52 root] (duquant.py 171): INFO === Start quantize layer 6 ===
[2025-05-13 23:30:54 root] (duquant.py 171): INFO === Start quantize layer 7 ===
[2025-05-13 23:30:56 root] (duquant.py 171): INFO === Start quantize layer 8 ===
[2025-05-13 23:30:58 root] (duquant.py 171): INFO === Start quantize layer 9 ===
[2025-05-13 23:31:01 root] (duquant.py 171): INFO === Start quantize layer 10 ===
[2025-05-13 23:31:03 root] (duquant.py 171): INFO === Start quantize layer 11 ===
[2025-05-13 23:31:05 root] (duquant.py 171): INFO === Start quantize layer 12 ===
[2025-05-13 23:31:07 root] (duquant.py 171): INFO === Start quantize layer 13 ===
[2025-05-13 23:31:09 root] (duquant.py 171): INFO === Start quantize layer 14 ===
[2025-05-13 23:31:11 root] (duquant.py 171): INFO === Start quantize layer 15 ===
[2025-05-13 23:31:13 root] (duquant.py 171): INFO === Start quantize layer 16 ===
[2025-05-13 23:31:15 root] (duquant.py 171): INFO === Start quantize layer 17 ===
[2025-05-13 23:31:18 root] (duquant.py 171): INFO === Start quantize layer 18 ===
[2025-05-13 23:31:20 root] (duquant.py 171): INFO === Start quantize layer 19 ===
[2025-05-13 23:31:22 root] (duquant.py 171): INFO === Start quantize layer 20 ===
[2025-05-13 23:31:24 root] (duquant.py 171): INFO === Start quantize layer 21 ===
[2025-05-13 23:31:26 root] (duquant.py 171): INFO === Start quantize layer 22 ===
[2025-05-13 23:31:28 root] (duquant.py 171): INFO === Start quantize layer 23 ===
[2025-05-13 23:31:31 root] (duquant.py 171): INFO === Start quantize layer 24 ===
[2025-05-13 23:31:33 root] (duquant.py 171): INFO === Start quantize layer 25 ===
[2025-05-13 23:31:35 root] (duquant.py 171): INFO === Start quantize layer 26 ===
[2025-05-13 23:31:38 root] (duquant.py 171): INFO === Start quantize layer 27 ===
[2025-05-13 23:31:40 root] (duquant.py 171): INFO === Start quantize layer 28 ===
[2025-05-13 23:31:43 root] (duquant.py 171): INFO === Start quantize layer 29 ===
[2025-05-13 23:31:45 root] (duquant.py 171): INFO === Start quantize layer 30 ===
[2025-05-13 23:31:47 root] (duquant.py 171): INFO === Start quantize layer 31 ===
[2025-05-13 23:31:50 root] (main.py 458): INFO 77.58072710037231
[2025-05-13 23:31:50 root] (main.py 481): INFO {'self_attn.k_proj': {'x_l2_diaq': 0.7454992542043328, 'x_relnorm_diaq': 0.12127150292508304, 'x_cos_diaq': 0.99268988519907, 'x_l2_rtn': 0.7454992542043328, 'x_relnorm_rtn': 0.12127150292508304, 'x_cos_rtn': 0.9926892518997192, 'wx_l2_diaq': 2.623023321852088, 'wx_relnorm_diaq': 0.03639071132056415, 'wx_cos_diaq': 0.999334953725338, 'wx_l2_rtn': 2.586500149220228, 'wx_relnorm_rtn': 0.03588881474570371, 'wx_cos_rtn': 0.9993349500000477}, 'self_attn.v_proj': {'x_l2_diaq': 0.7454992542043328, 'x_relnorm_diaq': 0.12127150292508304, 'x_cos_diaq': 0.99268988519907, 'x_l2_rtn': 0.7454992542043328, 'x_relnorm_rtn': 0.12127150292508304, 'x_cos_rtn': 0.9926892518997192, 'wx_l2_diaq': 0.9431248316541314, 'wx_relnorm_diaq': 0.11311921034939587, 'wx_cos_diaq': 0.9934838805347681, 'wx_l2_rtn': 0.9471733814571053, 'wx_relnorm_rtn': 0.11360429343767464, 'wx_cos_rtn': 0.9934832882136106}, 'self_attn.q_proj': {'x_l2_diaq': 0.7454992542043328, 'x_relnorm_diaq': 0.12127150292508304, 'x_cos_diaq': 0.99268988519907, 'x_l2_rtn': 0.7454992542043328, 'x_relnorm_rtn': 0.12127150292508304, 'x_cos_rtn': 0.9926892518997192, 'wx_l2_diaq': 3.2973448019474745, 'wx_relnorm_diaq': 0.04382492360309698, 'wx_cos_diaq': 0.99900808557868, 'wx_l2_rtn': 3.2637764792889357, 'wx_relnorm_rtn': 0.04346059312229045, 'wx_cos_rtn': 0.999007998034358}, 'self_attn.o_proj': {'x_l2_diaq': 0.25115647725760937, 'x_relnorm_diaq': 0.12147958227433264, 'x_cos_diaq': 0.9926603157073259, 'x_l2_rtn': 0.25115647725760937, 'x_relnorm_rtn': 0.12147958227433264, 'x_cos_rtn': 0.9926597308367491, 'wx_l2_diaq': 0.5068258913233876, 'wx_relnorm_diaq': 0.1576032149605453, 'wx_cos_diaq': 0.9872860983014107, 'wx_l2_rtn': 0.5068224382121116, 'wx_relnorm_rtn': 0.1576020895736292, 'wx_cos_rtn': 0.9872852545231581}, 'mlp.gate_proj': {'x_l2_diaq': 0.5466084694489837, 'x_relnorm_diaq': 0.12144735455513, 'x_cos_diaq': 0.9926683884114027, 'x_l2_rtn': 0.5466084694489837, 'x_relnorm_rtn': 0.12144735455513, 'x_cos_rtn': 0.9926678333431482, 'wx_l2_diaq': 4.16862802579999, 'wx_relnorm_diaq': 0.08450784103479236, 'wx_cos_diaq': 0.9963208176195621, 'wx_l2_rtn': 4.178792420774698, 'wx_relnorm_rtn': 0.08474026271142066, 'wx_cos_rtn': 0.9963205717504025}, 'mlp.down_proj': {'x_l2_diaq': 0.20888714399188757, 'x_relnorm_diaq': 0.13881207536906004, 'x_cos_diaq': 0.9904351588338614, 'x_l2_rtn': 0.20888714399188757, 'x_relnorm_rtn': 0.13881207536906004, 'x_cos_rtn': 0.9904341399669647, 'wx_l2_diaq': 0.7571406438946724, 'wx_relnorm_diaq': 0.18313155928626657, 'wx_cos_diaq': 0.982826191931963, 'wx_l2_rtn': 0.7621289258822799, 'wx_relnorm_rtn': 0.18456160463392735, 'wx_cos_rtn': 0.9828244242817163}, 'mlp.up_proj': {'x_l2_diaq': 0.5466084694489837, 'x_relnorm_diaq': 0.12144735455513, 'x_cos_diaq': 0.9926683884114027, 'x_l2_rtn': 0.5466084694489837, 'x_relnorm_rtn': 0.12144735455513, 'x_cos_rtn': 0.9926678333431482, 'wx_l2_diaq': 0.9066668171435595, 'wx_relnorm_diaq': 0.11134850955568254, 'wx_cos_diaq': 0.9936122708022594, 'wx_l2_rtn': 0.9103213008493185, 'wx_relnorm_rtn': 0.11181473243050277, 'wx_cos_rtn': 0.9936118759214878}}
[2025-05-13 23:31:50 root] (main.py 496): INFO {'x_l2_diaq': 0.5413940461086375, 'x_relnorm_diaq': 0.12385726793270026, 'x_cos_diaq': 0.9923574152801719, 'x_l2_rtn': 0.5413940461086375, 'x_relnorm_rtn': 0.12385726793270026, 'x_cos_rtn': 0.9923567561698812, 'wx_l2_diaq': 1.8861077619450433, 'wx_relnorm_diaq': 0.10427513858719197, 'wx_cos_diaq': 0.9931246140705687, 'wx_l2_rtn': 1.8793592993835253, 'wx_relnorm_rtn': 0.10452462723644983, 'wx_cos_rtn': 0.9931240518178258}
[2025-05-13 23:39:09 root] (main.py 143): INFO wikitext2 : 8.21834659576416
[2025-05-14 00:41:12 root] (main.py 345): INFO Namespace(rts=0.5, my_file_name='llama3_diaq_error', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-14 00:41:16 root] (main.py 429): INFO === start quantization ===
[2025-05-14 00:41:16 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-14 00:41:16 root] (duquant.py 45): INFO Starting ...
[2025-05-14 00:41:19 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-14 00:41:22 root] (duquant.py 171): INFO === Start quantize layer 1 ===
[2025-05-14 00:41:24 root] (duquant.py 171): INFO === Start quantize layer 2 ===
[2025-05-14 00:41:28 root] (duquant.py 171): INFO === Start quantize layer 3 ===
[2025-05-14 00:41:31 root] (duquant.py 171): INFO === Start quantize layer 4 ===
[2025-05-14 00:41:33 root] (duquant.py 171): INFO === Start quantize layer 5 ===
[2025-05-14 00:41:36 root] (duquant.py 171): INFO === Start quantize layer 6 ===
[2025-05-14 00:41:38 root] (duquant.py 171): INFO === Start quantize layer 7 ===
[2025-05-14 00:41:40 root] (duquant.py 171): INFO === Start quantize layer 8 ===
[2025-05-14 00:41:43 root] (duquant.py 171): INFO === Start quantize layer 9 ===
[2025-05-14 00:41:45 root] (duquant.py 171): INFO === Start quantize layer 10 ===
[2025-05-14 00:41:48 root] (duquant.py 171): INFO === Start quantize layer 11 ===
[2025-05-14 00:41:50 root] (duquant.py 171): INFO === Start quantize layer 12 ===
[2025-05-14 00:41:53 root] (duquant.py 171): INFO === Start quantize layer 13 ===
[2025-05-14 00:41:55 root] (duquant.py 171): INFO === Start quantize layer 14 ===
[2025-05-14 00:41:57 root] (duquant.py 171): INFO === Start quantize layer 15 ===
[2025-05-14 00:42:00 root] (duquant.py 171): INFO === Start quantize layer 16 ===
[2025-05-14 00:42:02 root] (duquant.py 171): INFO === Start quantize layer 17 ===
[2025-05-14 00:42:05 root] (duquant.py 171): INFO === Start quantize layer 18 ===
[2025-05-14 00:42:07 root] (duquant.py 171): INFO === Start quantize layer 19 ===
[2025-05-14 00:42:09 root] (duquant.py 171): INFO === Start quantize layer 20 ===
[2025-05-14 00:42:12 root] (duquant.py 171): INFO === Start quantize layer 21 ===
[2025-05-14 00:42:14 root] (duquant.py 171): INFO === Start quantize layer 22 ===
[2025-05-14 00:42:17 root] (duquant.py 171): INFO === Start quantize layer 23 ===
[2025-05-14 00:42:19 root] (duquant.py 171): INFO === Start quantize layer 24 ===
[2025-05-14 00:42:22 root] (duquant.py 171): INFO === Start quantize layer 25 ===
[2025-05-14 00:42:24 root] (duquant.py 171): INFO === Start quantize layer 26 ===
[2025-05-14 00:42:26 root] (duquant.py 171): INFO === Start quantize layer 27 ===
[2025-05-14 00:42:29 root] (duquant.py 171): INFO === Start quantize layer 28 ===
[2025-05-14 00:42:32 root] (duquant.py 171): INFO === Start quantize layer 29 ===
[2025-05-14 00:42:36 root] (duquant.py 171): INFO === Start quantize layer 30 ===
[2025-05-14 00:42:38 root] (duquant.py 171): INFO === Start quantize layer 31 ===
[2025-05-14 00:42:41 root] (main.py 458): INFO 85.5632426738739
[2025-05-14 00:42:41 root] (main.py 481): INFO {'self_attn.k_proj': {'x_l2_diaq': 0.7386425947770476, 'x_relnorm_diaq': 0.12286981963552535, 'x_cos_diaq': 0.9926146846264601, 'x_l2_rtn': 0.7386425947770476, 'x_relnorm_rtn': 0.12286981963552535, 'x_cos_rtn': 0.9926140327006578, 'wx_l2_diaq': 2.767446581274271, 'wx_relnorm_diaq': 0.03765324506093748, 'wx_cos_diaq': 0.9993772432208061, 'wx_l2_rtn': 2.5570673663169146, 'wx_relnorm_rtn': 0.03478051963611506, 'wx_cos_rtn': 0.9993771892040968}, 'self_attn.v_proj': {'x_l2_diaq': 0.7386425947770476, 'x_relnorm_diaq': 0.12286981963552535, 'x_cos_diaq': 0.9926146846264601, 'x_l2_rtn': 0.7386425947770476, 'x_relnorm_rtn': 0.12286981963552535, 'x_cos_rtn': 0.9926140327006578, 'wx_l2_diaq': 0.9249627655372024, 'wx_relnorm_diaq': 0.1120506920851767, 'wx_cos_diaq': 0.9935905858874321, 'wx_l2_rtn': 0.9292398225516081, 'wx_relnorm_rtn': 0.11257741856388748, 'wx_cos_rtn': 0.9935900289565325}, 'self_attn.q_proj': {'x_l2_diaq': 0.7386425947770476, 'x_relnorm_diaq': 0.12286981963552535, 'x_cos_diaq': 0.9926146846264601, 'x_l2_rtn': 0.7386425947770476, 'x_relnorm_rtn': 0.12286981963552535, 'x_cos_rtn': 0.9926140327006578, 'wx_l2_diaq': 3.4169968217611313, 'wx_relnorm_diaq': 0.045244527427712455, 'wx_cos_diaq': 0.9990312643349171, 'wx_l2_rtn': 3.2251647263765335, 'wx_relnorm_rtn': 0.04297125685843639, 'wx_cos_rtn': 0.9990311954170465}, 'self_attn.o_proj': {'x_l2_diaq': 0.24453499354422092, 'x_relnorm_diaq': 0.05033585208002478, 'x_cos_diaq': 0.9928289446979761, 'x_l2_rtn': 0.24453499354422092, 'x_relnorm_rtn': 0.05033585208002478, 'x_cos_rtn': 0.9928283635526896, 'wx_l2_diaq': 0.48617521091364324, 'wx_relnorm_diaq': 0.15496771468315274, 'wx_cos_diaq': 0.9875815566629171, 'wx_l2_rtn': 0.4861759433988482, 'wx_relnorm_rtn': 0.15496757917571813, 'wx_cos_rtn': 0.9875805992633104}, 'mlp.gate_proj': {'x_l2_diaq': 0.5343102999031544, 'x_relnorm_diaq': 0.12303346465341747, 'x_cos_diaq': 0.9925913363695145, 'x_l2_rtn': 0.5343102999031544, 'x_relnorm_rtn': 0.12303346465341747, 'x_cos_rtn': 0.9925907831639051, 'wx_l2_diaq': 4.073908830061555, 'wx_relnorm_diaq': 0.08196809515357018, 'wx_cos_diaq': 0.9965945966541767, 'wx_l2_rtn': 4.052677225321531, 'wx_relnorm_rtn': 0.0817097764229402, 'wx_cos_rtn': 0.9965943619608879}, 'mlp.down_proj': {'x_l2_diaq': 0.19945475389249623, 'x_relnorm_diaq': 0.13991073099896312, 'x_cos_diaq': 0.9903837814927101, 'x_l2_rtn': 0.19945475389249623, 'x_relnorm_rtn': 0.13991073099896312, 'x_cos_rtn': 0.9903827346861362, 'wx_l2_diaq': 0.6983514181338251, 'wx_relnorm_diaq': 0.1823517312295735, 'wx_cos_diaq': 0.9826481863856316, 'wx_l2_rtn': 0.7039173650555313, 'wx_relnorm_rtn': 0.18454611976630986, 'wx_cos_rtn': 0.9826464373618364}, 'mlp.up_proj': {'x_l2_diaq': 0.5343102999031544, 'x_relnorm_diaq': 0.12303346465341747, 'x_cos_diaq': 0.9925913363695145, 'x_l2_rtn': 0.5343102999031544, 'x_relnorm_rtn': 0.12303346465341747, 'x_cos_rtn': 0.9925907831639051, 'wx_l2_diaq': 0.891377829015255, 'wx_relnorm_diaq': 0.11229812609963119, 'wx_cos_diaq': 0.9934809245169163, 'wx_l2_rtn': 0.8948478493839502, 'wx_relnorm_rtn': 0.11282697971910238, 'wx_cos_rtn': 0.9934805203229189}}
[2025-05-14 00:42:41 root] (main.py 496): INFO {'x_l2_diaq': 0.5326483045105955, 'x_relnorm_diaq': 0.11498899589891412, 'x_cos_diaq': 0.9923199218298707, 'x_l2_rtn': 0.5326483045105955, 'x_relnorm_rtn': 0.11498899589891412, 'x_cos_rtn': 0.9923192518098014, 'wx_l2_diaq': 1.8941742080995547, 'wx_relnorm_diaq': 0.10379059024853632, 'wx_cos_diaq': 0.993186336808971, 'wx_l2_rtn': 1.8355843283435596, 'wx_relnorm_rtn': 0.10348280716321565, 'wx_cos_rtn': 0.9931857617838042}
[2025-05-14 00:42:53 root] (main.py 105): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-05-14 00:49:51 root] (main.py 143): INFO wikitext2 : 8.085163116455078
[2025-05-14 00:57:21 root] (main.py 345): INFO Namespace(rts=0.5, my_file_name='llama3_diaq_error', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-14 00:57:23 root] (main.py 429): INFO === start quantization ===
[2025-05-14 00:57:23 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-14 00:57:23 root] (duquant.py 45): INFO Starting ...
[2025-05-14 00:57:26 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-14 01:01:20 root] (main.py 345): INFO Namespace(rts=0.5, my_file_name='llama3_diaq_error', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-14 01:01:23 root] (main.py 429): INFO === start quantization ===
[2025-05-14 01:01:23 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-14 01:01:23 root] (duquant.py 45): INFO Starting ...
[2025-05-14 01:01:26 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-14 01:01:29 root] (duquant.py 171): INFO === Start quantize layer 1 ===
[2025-05-14 01:01:31 root] (duquant.py 171): INFO === Start quantize layer 2 ===
[2025-05-14 01:01:33 root] (duquant.py 171): INFO === Start quantize layer 3 ===
[2025-05-14 01:01:35 root] (duquant.py 171): INFO === Start quantize layer 4 ===
[2025-05-14 01:01:38 root] (duquant.py 171): INFO === Start quantize layer 5 ===
[2025-05-14 01:01:40 root] (duquant.py 171): INFO === Start quantize layer 6 ===
[2025-05-14 01:01:42 root] (duquant.py 171): INFO === Start quantize layer 7 ===
[2025-05-14 01:01:45 root] (duquant.py 171): INFO === Start quantize layer 8 ===
[2025-05-14 01:01:47 root] (duquant.py 171): INFO === Start quantize layer 9 ===
[2025-05-14 01:01:49 root] (duquant.py 171): INFO === Start quantize layer 10 ===
[2025-05-14 01:01:52 root] (duquant.py 171): INFO === Start quantize layer 11 ===
[2025-05-14 01:01:54 root] (duquant.py 171): INFO === Start quantize layer 12 ===
[2025-05-14 01:01:56 root] (duquant.py 171): INFO === Start quantize layer 13 ===
[2025-05-14 01:01:59 root] (duquant.py 171): INFO === Start quantize layer 14 ===
[2025-05-14 01:02:01 root] (duquant.py 171): INFO === Start quantize layer 15 ===
[2025-05-14 01:02:03 root] (duquant.py 171): INFO === Start quantize layer 16 ===
[2025-05-14 01:02:06 root] (duquant.py 171): INFO === Start quantize layer 17 ===
[2025-05-14 01:02:08 root] (duquant.py 171): INFO === Start quantize layer 18 ===
[2025-05-14 01:02:10 root] (duquant.py 171): INFO === Start quantize layer 19 ===
[2025-05-14 01:02:13 root] (duquant.py 171): INFO === Start quantize layer 20 ===
[2025-05-14 01:02:15 root] (duquant.py 171): INFO === Start quantize layer 21 ===
[2025-05-14 01:02:17 root] (duquant.py 171): INFO === Start quantize layer 22 ===
[2025-05-14 01:02:20 root] (duquant.py 171): INFO === Start quantize layer 23 ===
[2025-05-14 01:02:22 root] (duquant.py 171): INFO === Start quantize layer 24 ===
[2025-05-14 01:02:24 root] (duquant.py 171): INFO === Start quantize layer 25 ===
[2025-05-14 01:02:27 root] (duquant.py 171): INFO === Start quantize layer 26 ===
[2025-05-14 01:02:29 root] (duquant.py 171): INFO === Start quantize layer 27 ===
[2025-05-14 01:02:31 root] (duquant.py 171): INFO === Start quantize layer 28 ===
[2025-05-14 01:02:34 root] (duquant.py 171): INFO === Start quantize layer 29 ===
[2025-05-14 01:02:36 root] (duquant.py 171): INFO === Start quantize layer 30 ===
[2025-05-14 01:02:38 root] (duquant.py 171): INFO === Start quantize layer 31 ===
[2025-05-14 01:02:41 root] (main.py 458): INFO 78.27954936027527
[2025-05-14 01:02:41 root] (main.py 481): INFO {'self_attn.k_proj': {'x_l2_diaq': 0.7274175332859159, 'x_relnorm_diaq': 0.12154406239278615, 'x_cos_diaq': 0.9926574509590864, 'x_l2_rtn': 0.7274175332859159, 'x_relnorm_rtn': 0.12154406239278615, 'x_cos_rtn': 0.9926568493247032, 'wx_l2_diaq': 2.559318494051695, 'wx_relnorm_diaq': 0.03401520595070906, 'wx_cos_diaq': 0.9994264896959066, 'wx_l2_rtn': 2.5170329436659813, 'wx_relnorm_rtn': 0.03345208408427425, 'wx_cos_rtn': 0.9994264729321003}, 'self_attn.v_proj': {'x_l2_diaq': 0.7274175332859159, 'x_relnorm_diaq': 0.12154406239278615, 'x_cos_diaq': 0.9926574509590864, 'x_l2_rtn': 0.7274175332859159, 'x_relnorm_rtn': 0.12154406239278615, 'x_cos_rtn': 0.9926568493247032, 'wx_l2_diaq': 0.9080360585357994, 'wx_relnorm_diaq': 0.11434621503576636, 'wx_cos_diaq': 0.9933260940015316, 'wx_l2_rtn': 0.911951788701117, 'wx_relnorm_rtn': 0.11484115105122328, 'wx_cos_rtn': 0.9933255612850189}, 'self_attn.q_proj': {'x_l2_diaq': 0.7274175332859159, 'x_relnorm_diaq': 0.12154406239278615, 'x_cos_diaq': 0.9926574509590864, 'x_l2_rtn': 0.7274175332859159, 'x_relnorm_rtn': 0.12154406239278615, 'x_cos_rtn': 0.9926568493247032, 'wx_l2_diaq': 3.2094282247126102, 'wx_relnorm_diaq': 0.04234481815365143, 'wx_cos_diaq': 0.9990832228213549, 'wx_l2_rtn': 3.173598248511553, 'wx_relnorm_rtn': 0.04195052740396932, 'wx_cos_rtn': 0.9990831762552261}, 'self_attn.o_proj': {'x_l2_diaq': 0.25400591758079827, 'x_relnorm_diaq': 0.12421822198666632, 'x_cos_diaq': 0.9923247490078211, 'x_l2_rtn': 0.25400591758079827, 'x_relnorm_rtn': 0.12421822198666632, 'x_cos_rtn': 0.9923241343349218, 'wx_l2_diaq': 0.5046894676052034, 'wx_relnorm_diaq': 0.15828128077555448, 'wx_cos_diaq': 0.9870283547788858, 'wx_l2_rtn': 0.504687073873356, 'wx_relnorm_rtn': 0.15827977273147553, 'wx_cos_rtn': 0.9870275650173426}, 'mlp.gate_proj': {'x_l2_diaq': 0.5221198620274663, 'x_relnorm_diaq': 0.12154724448919296, 'x_cos_diaq': 0.9926583021879196, 'x_l2_rtn': 0.5221198620274663, 'x_relnorm_rtn': 0.12154724448919296, 'x_cos_rtn': 0.9926577527076006, 'wx_l2_diaq': 3.936096116900444, 'wx_relnorm_diaq': 0.08262357127387077, 'wx_cos_diaq': 0.9964831713587046, 'wx_l2_rtn': 3.9443763997405767, 'wx_relnorm_rtn': 0.08284223277587444, 'wx_cos_rtn': 0.9964829217642546}, 'mlp.down_proj': {'x_l2_diaq': 0.19933995744213462, 'x_relnorm_diaq': 0.13866943772882223, 'x_cos_diaq': 0.9904530849307775, 'x_l2_rtn': 0.19933995744213462, 'x_relnorm_rtn': 0.13866943772882223, 'x_cos_rtn': 0.9904520399868488, 'wx_l2_diaq': 0.7163706426508725, 'wx_relnorm_diaq': 0.18197055906057358, 'wx_cos_diaq': 0.9829004108905792, 'wx_l2_rtn': 0.7208158150315285, 'wx_relnorm_rtn': 0.18338749534450471, 'wx_cos_rtn': 0.9828987699002028}, 'mlp.up_proj': {'x_l2_diaq': 0.5221198620274663, 'x_relnorm_diaq': 0.12154724448919296, 'x_cos_diaq': 0.9926583021879196, 'x_l2_rtn': 0.5221198620274663, 'x_relnorm_rtn': 0.12154724448919296, 'x_cos_rtn': 0.9926577527076006, 'wx_l2_diaq': 0.8728057872503996, 'wx_relnorm_diaq': 0.11221949115861207, 'wx_cos_diaq': 0.993484677746892, 'wx_l2_rtn': 0.876373827457428, 'wx_relnorm_rtn': 0.11269810702651739, 'wx_cos_rtn': 0.9934842549264431}}
[2025-05-14 01:02:41 root] (main.py 496): INFO {'x_l2_diaq': 0.5256911712765161, 'x_relnorm_diaq': 0.12437347655317613, 'x_cos_diaq': 0.9922952558845282, 'x_l2_rtn': 0.5256911712765161, 'x_relnorm_rtn': 0.12437347655317613, 'x_cos_rtn': 0.9922946039587259, 'wx_l2_diaq': 1.8152492559581463, 'wx_relnorm_diaq': 0.10368587734410539, 'wx_cos_diaq': 0.9931046316134078, 'wx_l2_rtn': 1.8069765852830773, 'wx_relnorm_rtn': 0.10392162434540556, 'wx_cos_rtn': 0.9931041031543698}
[2025-05-14 01:02:53 root] (main.py 105): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-05-14 01:09:42 root] (main.py 143): INFO wikitext2 : 8.082250595092773
