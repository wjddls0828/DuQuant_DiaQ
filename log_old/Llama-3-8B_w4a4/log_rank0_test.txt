[2025-05-13 21:42:31 root] (main.py 345): INFO Namespace(rts=0.3, my_file_name='test', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 21:42:34 root] (main.py 429): INFO === start quantization ===
[2025-05-13 21:42:34 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 21:42:34 root] (duquant.py 45): INFO Starting ...
[2025-05-13 21:42:36 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 21:43:52 root] (main.py 345): INFO Namespace(rts=0.3, my_file_name='test', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 21:43:56 root] (main.py 429): INFO === start quantization ===
[2025-05-13 21:43:56 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 21:43:56 root] (duquant.py 45): INFO Starting ...
[2025-05-13 21:43:57 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 21:44:43 root] (main.py 345): INFO Namespace(rts=0.3, my_file_name='test', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 21:44:46 root] (main.py 429): INFO === start quantization ===
[2025-05-13 21:44:46 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 21:44:46 root] (duquant.py 45): INFO Starting ...
[2025-05-13 21:44:48 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 21:45:27 root] (main.py 345): INFO Namespace(rts=0.3, my_file_name='test', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 21:45:30 root] (main.py 429): INFO === start quantization ===
[2025-05-13 21:45:30 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 21:45:30 root] (duquant.py 45): INFO Starting ...
[2025-05-13 21:45:32 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 21:45:52 root] (main.py 345): INFO Namespace(rts=0.3, my_file_name='test', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 21:45:55 root] (main.py 429): INFO === start quantization ===
[2025-05-13 21:45:55 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 21:45:55 root] (duquant.py 45): INFO Starting ...
[2025-05-13 21:45:58 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 21:47:23 root] (main.py 345): INFO Namespace(rts=0.3, my_file_name='test', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 21:47:27 root] (main.py 429): INFO === start quantization ===
[2025-05-13 21:47:27 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 21:47:27 root] (duquant.py 45): INFO Starting ...
[2025-05-13 21:47:29 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 21:48:45 root] (main.py 345): INFO Namespace(rts=0.3, my_file_name='test', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 21:48:49 root] (main.py 429): INFO === start quantization ===
[2025-05-13 21:48:49 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 21:48:49 root] (duquant.py 45): INFO Starting ...
[2025-05-13 21:48:51 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 21:50:11 root] (main.py 345): INFO Namespace(rts=0.3, my_file_name='test', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 21:50:14 root] (main.py 429): INFO === start quantization ===
[2025-05-13 21:50:14 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 21:50:14 root] (duquant.py 45): INFO Starting ...
[2025-05-13 21:50:16 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 21:50:19 root] (duquant.py 171): INFO === Start quantize layer 1 ===
[2025-05-13 21:50:21 root] (duquant.py 171): INFO === Start quantize layer 2 ===
[2025-05-13 21:50:24 root] (duquant.py 171): INFO === Start quantize layer 3 ===
[2025-05-13 21:50:26 root] (duquant.py 171): INFO === Start quantize layer 4 ===
[2025-05-13 21:50:28 root] (duquant.py 171): INFO === Start quantize layer 5 ===
[2025-05-13 21:50:30 root] (duquant.py 171): INFO === Start quantize layer 6 ===
[2025-05-13 21:50:33 root] (duquant.py 171): INFO === Start quantize layer 7 ===
[2025-05-13 21:50:35 root] (duquant.py 171): INFO === Start quantize layer 8 ===
[2025-05-13 21:50:38 root] (duquant.py 171): INFO === Start quantize layer 9 ===
[2025-05-13 21:50:40 root] (duquant.py 171): INFO === Start quantize layer 10 ===
[2025-05-13 21:50:43 root] (duquant.py 171): INFO === Start quantize layer 11 ===
[2025-05-13 21:50:45 root] (duquant.py 171): INFO === Start quantize layer 12 ===
[2025-05-13 21:50:48 root] (duquant.py 171): INFO === Start quantize layer 13 ===
[2025-05-13 21:50:50 root] (duquant.py 171): INFO === Start quantize layer 14 ===
[2025-05-13 21:50:52 root] (duquant.py 171): INFO === Start quantize layer 15 ===
[2025-05-13 21:50:55 root] (duquant.py 171): INFO === Start quantize layer 16 ===
[2025-05-13 21:50:57 root] (duquant.py 171): INFO === Start quantize layer 17 ===
[2025-05-13 21:51:00 root] (duquant.py 171): INFO === Start quantize layer 18 ===
[2025-05-13 21:51:02 root] (duquant.py 171): INFO === Start quantize layer 19 ===
[2025-05-13 21:51:04 root] (duquant.py 171): INFO === Start quantize layer 20 ===
[2025-05-13 21:51:07 root] (duquant.py 171): INFO === Start quantize layer 21 ===
[2025-05-13 21:51:09 root] (duquant.py 171): INFO === Start quantize layer 22 ===
[2025-05-13 21:51:12 root] (duquant.py 171): INFO === Start quantize layer 23 ===
[2025-05-13 21:51:14 root] (duquant.py 171): INFO === Start quantize layer 24 ===
[2025-05-13 21:51:17 root] (duquant.py 171): INFO === Start quantize layer 25 ===
[2025-05-13 21:51:19 root] (duquant.py 171): INFO === Start quantize layer 26 ===
[2025-05-13 21:51:22 root] (duquant.py 171): INFO === Start quantize layer 27 ===
[2025-05-13 21:51:24 root] (duquant.py 171): INFO === Start quantize layer 28 ===
[2025-05-13 21:51:27 root] (duquant.py 171): INFO === Start quantize layer 29 ===
[2025-05-13 21:51:29 root] (duquant.py 171): INFO === Start quantize layer 30 ===
[2025-05-13 21:51:32 root] (duquant.py 171): INFO === Start quantize layer 31 ===
[2025-05-13 21:51:35 root] (main.py 458): INFO 80.52565860748291
[2025-05-13 21:51:35 root] (main.py 481): INFO {'self_attn.k_proj': {'x_l2_diaq': 0.7365953978151083, 'x_relnorm_diaq': 0.1212670924142003, 'x_cos_diaq': 0.9926906116306782, 'x_l2_rtn': 0.7365953978151083, 'x_relnorm_rtn': 0.1212670924142003, 'x_cos_rtn': 0.9926899801939726, 'wx_l2_diaq': 2.5892161298543215, 'wx_relnorm_diaq': 0.03520441739237867, 'wx_cos_diaq': 0.9993813391774893, 'wx_l2_rtn': 2.550863368436694, 'wx_relnorm_rtn': 0.03468240379879717, 'wx_cos_rtn': 0.999381335452199}, 'self_attn.v_proj': {'x_l2_diaq': 0.7365953978151083, 'x_relnorm_diaq': 0.1212670924142003, 'x_cos_diaq': 0.9926906116306782, 'x_l2_rtn': 0.7365953978151083, 'x_relnorm_rtn': 0.1212670924142003, 'x_cos_rtn': 0.9926899801939726, 'wx_l2_diaq': 0.9201776003465056, 'wx_relnorm_diaq': 0.11053629009984434, 'wx_cos_diaq': 0.9937724657356739, 'wx_l2_rtn': 0.9240020676515996, 'wx_relnorm_rtn': 0.11099963774904609, 'wx_cos_rtn': 0.9937720857560635}, 'self_attn.q_proj': {'x_l2_diaq': 0.7365953978151083, 'x_relnorm_diaq': 0.1212670924142003, 'x_cos_diaq': 0.9926906116306782, 'x_l2_rtn': 0.7365953978151083, 'x_relnorm_rtn': 0.1212670924142003, 'x_cos_rtn': 0.9926899801939726, 'wx_l2_diaq': 3.2505839373916388, 'wx_relnorm_diaq': 0.04315816820599139, 'wx_cos_diaq': 0.9990442991256714, 'wx_l2_rtn': 3.216450171545148, 'wx_relnorm_rtn': 0.04278612912457902, 'wx_cos_rtn': 0.9990442302078009}, 'self_attn.o_proj': {'x_l2_diaq': 0.243468364700675, 'x_relnorm_diaq': 0.12263968354091048, 'x_cos_diaq': 0.9925208650529385, 'x_l2_rtn': 0.243468364700675, 'x_relnorm_rtn': 0.12263968354091048, 'x_cos_rtn': 0.9925202913582325, 'wx_l2_diaq': 0.4872168553993106, 'wx_relnorm_diaq': 0.1545923122903332, 'wx_cos_diaq': 0.987693190574646, 'wx_l2_rtn': 0.48721490940079093, 'wx_relnorm_rtn': 0.15459178341552615, 'wx_cos_rtn': 0.9876922499388456}, 'mlp.gate_proj': {'x_l2_diaq': 0.5339137129485607, 'x_relnorm_diaq': 0.12184249749407172, 'x_cos_diaq': 0.9926223438233137, 'x_l2_rtn': 0.5339137129485607, 'x_relnorm_rtn': 0.12184249749407172, 'x_cos_rtn': 0.9926218017935753, 'wx_l2_diaq': 4.044668339192867, 'wx_relnorm_diaq': 0.08193577220663428, 'wx_cos_diaq': 0.9965379349887371, 'wx_l2_rtn': 4.052515164017677, 'wx_relnorm_rtn': 0.08214361395221204, 'wx_cos_rtn': 0.9965376760810614}, 'mlp.down_proj': {'x_l2_diaq': 0.2033881824463606, 'x_relnorm_diaq': 0.13934957794845104, 'x_cos_diaq': 0.9903579372912645, 'x_l2_rtn': 0.2033881824463606, 'x_relnorm_rtn': 0.13934957794845104, 'x_cos_rtn': 0.9903569035232067, 'wx_l2_diaq': 0.7281658304855227, 'wx_relnorm_diaq': 0.18124170787632465, 'wx_cos_diaq': 0.983066787943244, 'wx_l2_rtn': 0.7327369283884764, 'wx_relnorm_rtn': 0.18266129470430315, 'wx_cos_rtn': 0.9830650575459003}, 'mlp.up_proj': {'x_l2_diaq': 0.5339137129485607, 'x_relnorm_diaq': 0.12184249749407172, 'x_cos_diaq': 0.9926223438233137, 'x_l2_rtn': 0.5339137129485607, 'x_relnorm_rtn': 0.12184249749407172, 'x_cos_rtn': 0.9926218017935753, 'wx_l2_diaq': 0.8898186050355434, 'wx_relnorm_diaq': 0.11064831912517548, 'wx_cos_diaq': 0.9936969988048077, 'wx_l2_rtn': 0.8933927230536938, 'wx_relnorm_rtn': 0.11111155594699085, 'wx_cos_rtn': 0.9936965685337782}}
[2025-05-13 21:51:35 root] (main.py 496): INFO {'x_l2_diaq': 0.5320671666413546, 'x_relnorm_diaq': 0.1242107905314437, 'x_cos_diaq': 0.9923136178404093, 'x_l2_rtn': 0.5320671666413546, 'x_relnorm_rtn': 0.1242107905314437, 'x_cos_rtn': 0.992312962721501, 'wx_l2_diaq': 1.8442638996722442, 'wx_relnorm_diaq': 0.10247385531381172, 'wx_cos_diaq': 0.9933132880500385, 'wx_l2_rtn': 1.83673933321344, 'wx_relnorm_rtn': 0.10271091695592206, 'wx_cos_rtn': 0.9933127433593784}
[2025-05-13 21:51:50 root] (main.py 105): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-05-13 22:58:26 root] (main.py 345): INFO Namespace(rts=0.5, my_file_name='test', model='/mnt/models/llama/llama-3/Llama-3-8B', cache_dir='./cache', output_dir='./log/Llama-3-8B_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.6, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=True, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='duquant')
[2025-05-13 22:58:29 root] (main.py 429): INFO === start quantization ===
[2025-05-13 22:58:29 root] (main.py 435): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-05-13 22:58:29 root] (duquant.py 45): INFO Starting ...
[2025-05-13 22:58:31 root] (duquant.py 171): INFO === Start quantize layer 0 ===
[2025-05-13 22:58:34 root] (duquant.py 171): INFO === Start quantize layer 1 ===
[2025-05-13 22:58:36 root] (duquant.py 171): INFO === Start quantize layer 2 ===
[2025-05-13 22:58:38 root] (duquant.py 171): INFO === Start quantize layer 3 ===
[2025-05-13 22:58:40 root] (duquant.py 171): INFO === Start quantize layer 4 ===
[2025-05-13 22:58:43 root] (duquant.py 171): INFO === Start quantize layer 5 ===
[2025-05-13 22:58:46 root] (duquant.py 171): INFO === Start quantize layer 6 ===
[2025-05-13 22:58:48 root] (duquant.py 171): INFO === Start quantize layer 7 ===
[2025-05-13 22:58:51 root] (duquant.py 171): INFO === Start quantize layer 8 ===
[2025-05-13 22:58:53 root] (duquant.py 171): INFO === Start quantize layer 9 ===
